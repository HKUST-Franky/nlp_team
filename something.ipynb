{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import qwen, kimi, gpt\n",
    "import util.mark as m\n",
    "import util.data_processing as dp\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen = qwen.Qwen()\n",
    "# #TODO add this to class\n",
    "# model_name = \"qwen\"\n",
    "# #TODO modify this to use the file in prompt dir\n",
    "\n",
    "model_list = {\n",
    "    \"qwen\": qwen.Qwen(), \n",
    "    \"kimi\": kimi.Kimi(), \n",
    "    \"gpt\": gpt.GPT()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fixed = \"\"\"Given a passage, identify any entity, relation, contradictory, subjective, unverifiable, or invented errors in the passage. Mark each erroneous segment by enclosing it within the corresponding <error></error> tags. If there are no errors, return the passage with no tags. Any identified errors should be highlighted using the tag <error> without altering the original text. Below are the error definitions of the error types.\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- Entity Error: A small part of a sentence, often an entity (e.g., location name), is incorrect (usually 1-3 words). Entity errors often involve noun phrases or nouns.\n",
    "- Relational Error: A sentence is partially incorrect due to a small part (usually 1-3 words). Relational errors often involve verbs and are often the opposite of what they should be.\n",
    "- Contradictory Sentence Error: A sentence where the entire content is contradicted by the given reference, meaning the sentence can be proven false due to a contradiction with information in the passage.\n",
    "- Invented Info Error: Errors referring to entities that are not known or do not exist. This does not include fictional characters in books or movies. - Invented errors include phrases or sentences with unknown entities or misleading information.\n",
    "- Subjective Sentence: An entire sentence or phrase that is subjective and cannot be verified, so it should not be included.\n",
    "- Unverifiable Sentence: A sentence where the whole sentence or phrase is unlikely to be factually grounded. Although it can be true, the sentence cannot be confirmed nor denied using the reference given or internet search. It is often something personal or private and hence cannot be confirmed.\n",
    "\n",
    "##\n",
    "Passage: Marooned on Mars is a science fiction novel aimed at a younger audience. It was written by Andy Weir and published by John C. Winston Co. in 1952, featuring illustrations by Alex Schomburg. It ended up having a readership of older boys despite efforts for it to be aimed at younger kids. The novel inspired the famous Broadway musical \"Stranded Stars,\" which won six Tony Awards. The novel tells a story of being stranded on the Purple Planet. I wish the novel had more exciting and thrilling plot twists.\n",
    "Edited: Marooned on Mars is a science fiction novel aimed at a younger audience.\n",
    "It was written by <error>Lester del Rey</error> and published by John C. Winston Co. in 1952, featuring illustrations by Alex Schomburg.\n",
    "<error>It ended up having a readership of older boys despite efforts for it to be aimed at younger kids.</error>\n",
    "<error>The novel inspired the famous Broadway musical \"Stranded Stars,\" which won six Tony Awards.</error>\n",
    "The novel tells a story of being stranded on the <error>Purple</error> Planet.\n",
    "<error>I wish the novel had more exciting and thrilling plot twists.</error>\n",
    "##\n",
    "\n",
    "Instructions: Now detect errors and include tag in the following passage as demonstrated in the example above. Use <error></error> tags around each identified error segment. If there are no errors, return the passage unchanged. Wait for my input after Passage:\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "# system_prompt\n",
    "SYS_PROMPT = \"You are a intelligent and clever expert on finding the hallucations errors in the text.\"\n",
    "\n",
    "#prompt for redo\n",
    "user_prompt = \"\"\"\n",
    "MY SWEET HEART, PLEASE DO NOT CHANGE THE ORIGINAL TEXT, JUST ADD TAGS, PLEASE. CAN YOU DO THAT AGAIN!\n",
    "Your output should be like: original text <error>original text</error> original text\n",
    "No extra text is needed. Just give me the answer.\n",
    "It should be exactly the same, including spaces\n",
    "This is the original:\" + prompt_user\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "Please give me the answer with thanks. Your output should be like: original text <error>original text</error> original text. No extra text is needed. Just give me the answer. It should be exactly the same, including spaces. This is the original text:\n",
    "```\n",
    "{text}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert the hard labels into soft labels\n",
    "def convert_into_softlabels(in_file: str, out_file: str):\n",
    "    data = pd.read_json(in_file, lines=True)\n",
    "    soft_labels_list = []\n",
    "    for ele in data[\"hard_labels\"]:\n",
    "        soft_labels = []\n",
    "        for start, end in ele:\n",
    "            soft_labels.append({\"start\": start, \"prob\": 1.0, \"end\": end})\n",
    "        soft_labels_list.append(soft_labels)\n",
    "    data[\"soft_labels\"] = soft_labels_list\n",
    "    data.to_json(out_file, orient=\"records\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text_list_with_mark_into_output_file(input_cor: List[str], text_lst: List[str], mark:m.Mark, name_feature:str):\n",
    "    output_lst = []\n",
    "    input_and_output = zip(input_cor, text_lst)\n",
    "    for tu in input_and_output:\n",
    "        #get the hard_label i.e. [[12, 34], [34, 55] ...]\n",
    "        #tu[1] text, tu[1] cor_input\n",
    "        hard_labels = m.starts_and_ends(tu[1], mark)\n",
    "        #TODO I dont know how to deal with the fucking soft labels, just blank\n",
    "        #There soft label is empty, and hash_labels is just we got above.\n",
    "        soft_labels = []\n",
    "        for hard_label in hard_labels:\n",
    "            soft_labels.append(dp.SoftLabel({\"start\": hard_label[0], \"prob\": 1.0, \"end\":hard_label[1]}))\n",
    "        labels = dp.Labels(soft_labels=soft_labels, hard_labels=hard_labels)\n",
    "        #We need to put the input we use, too!\n",
    "        #one instance\n",
    "        output_one = dp.Output(tu[0] | labels)\n",
    "        #add it to the list\n",
    "        output_lst.append(output_one)\n",
    "        print(output_lst)\n",
    "    #put them into a file!, you can specific the file_name actually\n",
    "\n",
    "    timestampe = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"./output/\" \n",
    "    output_filename = name_feature + timestampe + str(random.randint(0, 1000))\n",
    "    suffix = \".jsonl\"\n",
    "    full_output_filename = output_dir + output_filename + suffix \n",
    "    dp.save_file_output(output_lst, full_output_filename) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO modify this to read a list of file in a dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = m.Mark(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    #load file\n",
    "    input_dir_path = \"./input_data/\" \n",
    "    suffix = \".jsonl\"\n",
    "    full_file_name = input_dir_path + file_name + suffix\n",
    "    input_lst = dp.load_file_jsonl(full_file_name)\n",
    "    \n",
    "    return input_lst\n",
    "    #get prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"ar\", \"de\", \"en\", \"es\", \"fi\", \"fr\", \"hi\", \"it\", \"sv\", \"zh\"]\n",
    "input_lst = load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_it(prompt_user_lst, mask, model):\n",
    "    meow_lst = []\n",
    "    for prompt_user in prompt_user_lst:\n",
    "        ask = lambda x : model.ask(prompt_fixed + x, SYS_PROMPT)\n",
    "        meow = ask(prompt_user)\n",
    "        cnt = 0\n",
    "        while m.plain_text(meow, mask) != prompt_user:\n",
    "            #TODO modify this log info\n",
    "            log_info = f\"log info: original input:{prompt_user}, gpt output:{m.plain_text(meow, mask)}\\n\"\n",
    "            log_info += f\"difference: {m.find_char_differences(prompt_user, m.plain_text(meow, mask))[:5]}\"\n",
    "            to_add = \"(0, '-  ') means you lost a space at the 0 position. (0, '+  ' mean you get a extra space at the 0 position). Just add the character at the corresponding position.\"\n",
    "            print(log_info)\n",
    "            meow = ask(prompt_user + log_info + to_add + user_prompt)        \n",
    "\n",
    "            # to prevent loop\n",
    "            #modify this part make it more concise\n",
    "            cnt += 1\n",
    "            if cnt > 3:\n",
    "                meow = prompt_user\n",
    "                break\n",
    "            \n",
    "        meow_lst.append(meow)\n",
    "    print(meow_lst)\n",
    "    return meow_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prompt_user_lst = list(input[\"model_output_text\"] for input in input_lst)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# meow_lst = ask_it(prompt_user_lst, mask, model_list[\"qwen\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prompt_user_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_lst\u001b[49m:\n\u001b[1;32m      6\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_output_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_lst' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_user_lst = list(input[\"model_output_text\"] for input in input_lst)\n",
    "meow_lst = ask_it(prompt_user_lst, mask, model_list[\"qwen\"])\n",
    "\n",
    "prompt_user_lst = []\n",
    "for input in input_lst:\n",
    "    question = input[\"model_input\"]\n",
    "    response = input[\"model_output_text\"]\n",
    "    prompt_input = \"Question:\" + question + '\\n' + \"Response:\" + response\n",
    "    #print(prompt_user_lst[10])\n",
    "    prompt_user_lst.append(prompt_input)\n",
    "\n",
    "print(prompt_user_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file have been saved to ./output/gpt_en_20241029_182407877.jsonl\n"
     ]
    }
   ],
   "source": [
    "transform_text_list_with_mark_into_output_file(input_lst, meow_lst, mask, \"gpt_en_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "ref_file = \"mushroom.ar-val.v2.jsonl\"\n",
    "pred_file = \"qwen_ar20241029_0330473.jsonl\"\n",
    "output_file = \"scores.txt\"\n",
    "\n",
    "def run_evaluation(ref_file: str, pred_file: str, output_file: str):\n",
    "    command = f\"python3 util/score.py {ref_file} {pred_file} {output_file}\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "run_evaluation(ref_file, pred_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
